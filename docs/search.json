[
  {
    "objectID": "posts/2025-02-06-Waiting_to_pass_exam.html",
    "href": "posts/2025-02-06-Waiting_to_pass_exam.html",
    "title": "How many times do I need to take a test to randomly get all questions correct?",
    "section": "",
    "text": "Darrin Rogers asked on Mastadon what are the “number of tries it would take, guessing randomly, to get 100% on a quiz if you had unlimited retries.” Here we will outline two ways to solve this problem: using a simulation and using a combination of the binomial and geometric distributions. Let’s consider an example of a 5 question test where each question has four options, hence the probability of getting any one question correct is 1/4.\n\nsize &lt;- 5 # Test size (i.e. number of questions)\np &lt;- 1/4 # Probability of randomly getting correct answer\n\nWe can use the sample function to simulate on test attempt.\n\ntest &lt;- sample(c(TRUE, FALSE), size = size, prob = c(p, 1 - p), replace = TRUE)\ntest\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\n\nNext, let’s write a function that will simulate repeatedly take a test until all the questions are correct. I have added an additional parameter stop_score which specifies the mean score on the test before stopping. This will allow us to modify the question to answer how many tests do I need to take to pass. For now, stop_score = 1 will continue until all questions are correct.\n\n#' Simulate how long until a specified number of responses are correct\n#' @param size test size.\n#' @param prob probability of randomly getting correct answer\n#' @param stop_score the score on the test we wish to achieve. Value of 1\n#'        indicates a perfect score.\nsimulate_test &lt;- function(size, p, stop_score = 1) {\n    n &lt;- 0\n    repeat{\n        n &lt;- n + 1\n        test &lt;- sample(c(TRUE, FALSE),\n                       size = size,\n                       prob = c(p, 1 - p),\n                       replace = TRUE)\n        if(mean(test) &gt;= stop_score) {\n            break\n        }\n    }\n    return(n)\n}\n\nWe can run one test to see how long we need to wait until all questions on the test were answered correctly.\n\n(num_tests &lt;- simulate_test(size = size, p = p))\n\n[1] 158\n\n\nFor this one simulation, it took 158 to randomly get all the questions correct. Let’s now run this simulation 1,000 times.\n\nsimulations &lt;- integer(1000)\nfor(i in 1:length(simulations)) {\n    simulations[i] &lt;- simulate_test(size = size, p = p)\n}\nmean(simulations)\n\n[1] 977.858\n\nmedian(simulations)\n\n[1] 687\n\n\nFor this simulation the average “wait time” until all questions were answered correctly is 977.858. Since the distribution is not symmetrical it may be more appropriate to use the median. Here, 50% of the simulations returned a perfect score in fewer than 687 attempts.\n\nggplot(data.frame(x = simulations), aes(x = x)) +\n    geom_histogram(aes(y = ..density..), bins = 50, fill = 'grey70') +\n    geom_density(color = 'blue') +\n    ggtitle('Distribution of simulation results')\n\n\n\n\n\n\n\n\nLet’s return to a single test attempt. We can use the binomial distribution to calculate the probability of getting k questions correct on this 5 question test.\n\ndist &lt;- dbinom(x = 0:size, size = size, prob = p)\nggplot(data.frame(x = 0:size,                          \n                  prob = dist,\n                  label = paste0(round(100 * dist, digits = 2), '%')),\n       aes(x = x, y = prob, label = label)) +\n    geom_bar(stat = 'identity', fill = 'grey50') +\n    geom_text(vjust = -0.5)\n\n\n\n\n\n\n\n\nThe probability of getting all 5 questions on this test is 0.0009766. We can now treat each test attempt as a Bernoulli trial where the probability of success is 0.0009766. The geometric distribution gives us the number of Bernoulli trials we need to get one success. The mean for the geometric distribution are:\n\\[ \\mu = \\frac{1}{p} \\]\nTherefore, it will take an average of 1024 test attempts before getting all questions correct on the attempt.\n\n(p_all_correct &lt;- dbinom(x = size, size = size, prob = p))\n\n[1] 0.0009765625\n\n1 / p_all_correct\n\n[1] 1024\n\n\nHowever, the geometric distribution is not symmetrical so using the mean not be desirable. Here is the geometric distribution for where the probability of success is 0.0009766.\n\ngeom_dist &lt;- data.frame(x = 0:5000,\n                        y = dgeom(0:5000, prob = dbinom(x = size, size = size, prob = p)))\ncut_point50 &lt;- qgeom(0.50, prob = dbinom(x = size, size = size, prob = p))\ncut_point95 &lt;- qgeom(0.95, prob = dbinom(x = size, size = size, prob = p))\nggplot(geom_dist, aes(x = x, y = y)) +\n    geom_polygon(data = rbind(data.frame(x = 0, y = 0),\n                              geom_dist[geom_dist$x &lt; cut_point95,],\n                              data.frame(x = cut_point95, y = 0)),\n                 fill = 'grey70') +\n    geom_polygon(data = rbind(data.frame(x = 0, y = 0),\n                              geom_dist[geom_dist$x &lt; cut_point50,],\n                              data.frame(x = cut_point50, y = 0)),\n                 fill = 'grey50') +\n    geom_path(stat = 'identity', color = 'blue')\n\n\n\n\n\n\n\n\nThe shaded area corresponds to 50% of the area. That is, if we conduct 709 tests we are 50% likely to get a test with all the answers correct. Want to be 95% sure to get a test with all answers correct, then administer 3066 tests.\nWe can tweak the question slightly: What is the average number of tests I would have to take before passing if the answers are randomly selected? For this example, I am considering getting 4 or 5 questions correct passing. We can get the probability of getting 4 or 5 questions correct from the binomial distribution, which is 0.015625.\n\np_pass &lt;- dbinom(x = 4:5, size = size, prob = p) |&gt; sum()\n1 / p_pass\n\n[1] 64\n\n\nTo just pass, we have to wait much less. We can also calculate this using the simulate_test function defined above.\n\nsimulations2 &lt;- integer(1000)\nfor(i in 1:length(simulations2)) {\n    simulations2[i] &lt;- simulate_test(size = size, p = p, stop_score = 0.8)\n}\nmean(simulations2)\n\n[1] 62.291\n\nmedian(simulations2)\n\n[1] 44\n\n\nOr using the geometric distribution:\n\nqgeom(0.50, prob = p_pass)\n\n[1] 44\n\nqgeom(0.95, prob = p_pass)\n\n[1] 190"
  },
  {
    "objectID": "course/software.html",
    "href": "course/software.html",
    "title": "Software",
    "section": "",
    "text": "R and RStudio\n\nWe will make use of R, an open source statistics program and language. Be sure to install R and RStudio on your own computers within the first few days of the class.\n\nR - download for Windows, Mac, or Linux.\nRStudio - Download Windows, Mac, or Linux versions from here\n\nIf using Windows, you also need to download RTools.\n\n\nLaTeX\n\nLaTeX is a typesetting language for preparing documents. Documents are written in plain text files. Formatting the document is done using specific markup. If you have used HTML, the framework is similar however instead of using &lt;TAG&gt;&lt;/TAG&gt; syntax, LaTeX uses \\TAG{} format. We will primarily use Markdown, and its extension R Markdown for preparing documents in this class. However, when preparing PDF documents, the Markdown will first be converted to LaTeX before creating the PDF file. As such, a LaTeX converter is necessary. There are LaTeX installers for Windows (MiKTeX) and Mac (BasicTeX). Alternatively, the tinytex R package provides an easier way of installing LaTeX directly from within R:\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()\n\n\n\nSource Control\nAll course materials will be made available on Github which provides an implementation of the git open source version control system. RStudio supports git directly, but I recommend downloading Sourcetree. This is a free desktop client that provides an easier interface for working with Github. You will also need to create an account on Github.\nFor more information, Jenny Bryan’s Happy Git and Github for the useR is a free online book covering the important features of source control for R users.\n\n\nR Packages\n\nOnce everything is installed, execute the following command in RStudio to install the packages we will use for this class (you can copy-and-paste):\n\ninstall.packages(c('openintro','devtools','tidyverse', 'ggplot2',\n                   'psych','reshape2','knitr','markdown','shiny','R.rsp',\n                   'fivethirtyeight'))\ndevtools::install_github(\"jbryer/DATA606\")\ndevtools::install_github('jbryer/VisualStats')",
    "crumbs": [
      "Course information",
      "Software"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DAV5300 - Computational Math and Statistics",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses.\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nPrepare\nSlides\nHomework\nLab\n\n\n\n\nTue, Jan 21\nIntro to the Course / Intro to Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Jan 28\nSummarizing Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Feb 04\nProbability and Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Feb 11\nFoundation for Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Feb 18\nInference for Categorical Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Feb 25\nInference for Numerical Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Mar 04\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Mar 11\nMultiple Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Mar 18\nMaximum Likelihood Estimation and Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTue, Mar 25\nPredictive Modeling and CART Methods\n\n\n\n\n\n\n\n\n\n\nTue, Apr 01\nConferences (online)\n\n\n\n\n\n\nTue, Apr 08\nSPRING BREAK - NO CLASS\n\n\n\n\n\n\nTue, Apr 15\nBayesian Analysis\n\n\n\n\n\n\n\n\n\n\nTue, Apr 22\nNO CLASS\n\n\n\n\n\n\nTue, Apr 29\nPresentations\n\n\n\n\n\n\nTue, May 06\nFinal Exam",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "course/materials.html",
    "href": "course/materials.html",
    "title": "Materials",
    "section": "",
    "text": "These are supplemental materials. Click to download.\n\nCheatsheets_2019.pdf\nClassical_Machine_Learning.JPG\nggplot_aesthetics_cheatsheet.png\nKruschke-Liddell2018_Article_BayesianDataAnalysisForNewcome.pdf\nR_Syntax_Comparison.jpeg\nRiezler2021_EmpiricalMethods-draft.pdf\nstats_handout.pdf\nTextbooks\n\nimstat.pdf\nlsr-0.6.pdf\nos4.pdf",
    "crumbs": [
      "Course information",
      "Materials"
    ]
  },
  {
    "objectID": "assignments/homework.html",
    "href": "assignments/homework.html",
    "title": "Homework",
    "section": "",
    "text": "The solutions to the practice problems are at the end of the book and do not need to be handed in. Graded assignments should use the provided R markdown templates provided below. Data for the homework assignments, and for within the chapters too, can be downloaded here. Or alternatively all the data is included in the openintro R packge (use the data(package = 'openintro') command to list all the datasets available in that package). Right click on the “Template” link and choose “Save link as…” to save the R markdown file to your computer. By default, the Rmarkdown files should generate PDFs. This is the preferred format since PDFs can be uploaded to Blackboard. See the software course page for instructions on installing LaTeX.\n\nChapter 1 - Introduction to Data (Template)\nChapter 2 - Summarizing Data (Template)\nChapter 3 - Probability (Template)\nChapter 4 - Distributions of Random Variables (Template)\nChapter 5 - Foundations for Inference (Template)\nChapter 6 - Inference for Categorical Data (Template)\nChapter 7 - Inference for Numerical Data (Template)\nChapter 8 - Introduction to Linear Regression (Template)\nChapter 9 - Multiple and Logistic Regression (Template)"
  },
  {
    "objectID": "course/instructor.html",
    "href": "course/instructor.html",
    "title": "Instructors",
    "section": "",
    "text": "Jason Bryer, Ph.D.\nEmail: jason.bryer@cuny.edu\nI am currently an Assistant Professor and Associate Director in the Data Science and Information Systems department at the City University of New York. I am also the Principal Investigator of the FIPSE ($3 million #P116F150077) and IES funded ($3.8 million R305A210269) Diagnostic Assessment and Achievement of College Skills (DAACS), which is a suite of technological and social supports designed to optimize student learning. My other research interests include quasi-experimental designs with an emphasis on propensity score analysis, data systems to support formative assessment, and the use of open source software for conducting reproducible research. I have authored over a dozen R packages, including three related to conducting propensity score analyses.\n\nContact\nOffice Hours before or after class or by appointment. You’re encouraged to schedule an appointment and I have time nearly everyday.\nFor the most part, you can expect me to respond to questions by email within 24 hours. If you do not hear back from me within 48 hours of sending an email, please resend your message. I will be checking in on the course regularly, just about every day and likely several times each day. Please do not hesitate to ask if you have questions or concerns.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "chapters/chapter8.html",
    "href": "chapters/chapter8.html",
    "title": "Chapter 8 - Linear Regression",
    "section": "",
    "text": "Learning Outcomes\n\nDefine the explanatory variable as the independent variable (predictor), and the response variable as the dependent variable (predicted).\nPlot the explanatory variable (\\(x\\)) on the x-axis and the response variable (\\(y\\)) on the y-axis, and fit a linear regression model $y = \\beta_0 + \\beta_1 x$ where \\(\\beta_0\\) is the intercept, and \\(\\beta_1\\) is the slope.\n\nNote that the point estimates (estimated from observed data) for \\(\\beta_0\\) and \\(\\beta_1\\) are \\(b_0\\) and \\(b_1\\), respectively.\n\nWhen describing the association between two numerical variables, evaluate\n\ndirection: positive (\\(x \\uparrow, y \\uparrow\\)), negative (\\(x \\downarrow, y \\uparrow\\))\nform: linear or not\nstrength: determined by the scatter around the underlying relationship\n\nDefine correlation as the association between two numerical variables.\n\nNote that a relationship that is nonlinear is simply called an association.\n\nNote that correlation coefficient (\\(r\\), also called Pearson’s \\(r\\)) the following properties:\n\nthe magnitude (absolute value) of the correlation coefficient measures the strength of the linear association between two numerical variables\nthe sign of the correlation coefficient indicates the direction of association\nthe correlation coefficient is always between -1 and 1, inclusive, with -1 indicating perfect negative linear association, +1 indicating perfect positive linear association, and 0 indicating no relationship\nthe correlation coefficient is unitless\nsince the correlation coefficient is unitless, it is not affected by changes in the center or scale of either variable (such as unit conversions)\nthe correlation of X with Y is the same as of Y with X\nthe correlation coefficient is sensitive to outliers\n\nRecall that correlation does not imply causation.\nDefine residual (\\(e\\)) as the difference between the observed (\\(y\\)) and predicted (\\(\\hat{y}\\)) values of the response variable. $e_i = y_i - \\hat{y}_i$\nDefine the least squares line as the line that minimizes the sum of the squared residuals, and list conditions necessary for fitting such line:\n\nlinearity\nnearly normal residuals\nconstant variability\n\nDefine an indicator variable as a binary explanatory variable (with two levels).\nCalculate the estimate for the slope (\\(b_1\\)) as $b_1 = R\\frac{s_y}{s_x}$, where \\(r\\) is the correlation coefficient, \\(s_y\\) is the standard deviation of the response variable, and \\(s_x\\) is the standard deviation of the explanatory variable.\nInterpret the slope as\n\n“For each unit increase in \\(x\\), we would expect \\(y\\) to increase/decrease on average by \\(|b_1|\\) units” when \\(x\\) is numerical.\n“The average increase/decrease in the response variable when between the baseline level and the other level of the explanatory variable is \\(|b_1|\\).” when \\(x\\) is categorical.\nNote that whether the response variable increases or decreases is determined by the sign of \\(b_1\\).\n\nNote that the least squares line always passes through the average of the response and explanatory variables (\\(\\bar{x},\\bar{y}\\)).\nUse the above property to calculate the estimate for the slope (\\(b_0\\)) as $b_0 = \\bar{y} - b_1 \\bar{x}$, where \\(b_1\\) is the slope, \\(\\bar{y}\\) is the average of the response variable, and \\(\\bar{x}\\) is the average of explanatory variable.\nInterpret the intercept as\n\n“When \\(x = 0\\), we would expect \\(y\\) to equal, on average, \\(b_0\\).” when \\(x\\) is numerical.\n“The expected average value of the response variable for the reference level of the explanatory variable is \\(b_0\\).” when \\(x\\) is categorical.\n\nPredict the value of the response variable for a given value of the explanatory variable, \\(x^\\star\\), by plugging in \\(x^\\star\\) in the in the linear model: $\\hat{y} = b_0 + b_1 x^\\star$\n\nOnly predict for values of \\(x^\\star\\) that are in the range of the observed data.\nDo not extrapolate beyond the range of the data, unless you are confident that the linear pattern continues.\n\nDefine \\(R^2\\) as the percentage of the variability in the response variable explained by the the explanatory variable.\n\nFor a good model, we would like this number to be as close to 100% as possible.\nThis value is calculated as the square of the correlation coefficient, and is between 0 and 1, inclusive.\n\nDefine a leverage point as a point that lies away from the center of the data in the horizontal direction.\nDefine an influential point as a point that influences (changes) the slope of the regression line.\n\nThis is usually a leverage point that is away from the trajectory of the rest of the data.\n\nDo not remove outliers from an analysis without good reason.\nBe cautious about using a categorical explanatory variable when one of the levels has very few observations, as these may act as influential points.\nDetermine whether an explanatory variable is a significant predictor for the response variable using the \\(t\\)-test and the associated p-value in the regression output.\nSet the null hypothesis testing for the significance of the predictor as $H_0: \\beta_1 = 0$, and recognize that the standard software output yields the p-value for the two-sided alternative hypothesis.\n\nNote that \\(\\beta_1 = 0\\) means the regression line is horizontal, hence suggesting that there is no relationship between the explanatory and the response variables.\n\nCalculate the T score for the hypothesis test as $T_{df}=\\frac { b_{ 1 }-{ null\\quad value } }{ SE_{ b_{ 1 } } }$ with \\(df = n - 2\\).\n\nNote that the T score has \\(n - 2\\) degrees of freedom since we lose one degree of freedom for each parameter we estimate, and in this case we estimate the intercept and the slope.\n\nNote that a hypothesis test for the intercept is often irrelevant since it’s usually out of the range of the data, and hence it is usually an extrapolation.\nCalculate a confidence interval for the slope as $b_1 \\pm t^\\star_{df} SE_{b_1}$ where \\(df = n - 2\\) and $t^\\star_{df}$ is the critical score associated with the given confidence level at the desired degrees of freedom.\n\nNote that the standard error of the slope estimate $SE_{b_1}$ can be found on the regression output.\n\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\nInteraction Terms\nRegerssion for non-linear terms\nLinear regression with SAT scores - This document outlines the implementation of linear regression step-by-step emphasizing visualizations.\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "8 - Linear Regression"
    ]
  },
  {
    "objectID": "chapters/chapter6.html",
    "href": "chapters/chapter6.html",
    "title": "Chapter 6 - Inference for Categorical Data",
    "section": "",
    "text": "Learning Outcomes\n\nDefine population proportion \\(p\\) (parameter) and sample proportion \\(\\hat{p}\\) (point estimate).\nCalculate the sampling variability of the proportion, the standard error, as [ SE = , ] where \\(p\\) is the population proportion.\n\nNote that when the population proportion \\(p\\) is not known (almost always), this can be estimated using the sample proportion, \\(SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).\n\nRecognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.\n\nIn the case of the proportion the CLT tells us that if \\\n\nthe observations in the sample are independent, \\\nthe sample size is sufficiently large (checked using the success/failure condition: \\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\)), \\ then the distribution of the sample proportion will be nearly normal, centered at the true population proportion and with a standard error of \\(\\sqrt{\\frac{p(1-p)}{n}}\\). [ N ( mean = p, SE = ) ]\n\n\nNote that if the CLT doesn?t apply and the sample proportion is low (close to 0) the sampling distribution will likely be right skewed, if the sample proportion is high (close to 1) the sampling distribution will likely be left skewed.\nRemember that confidence intervals are calculated as [ ] and test statistics are calculated as [ ]\nNote that the standard error calculation for the confidence interval and the hypothesis test are different when dealing with proportions, since in the hypothesis test we need to assume that the null hypothesis is true – remember: p-value = P(observed or more extreme test statistic \\(|\\) \\(H_0\\) true).\n\nFor confidence intervals use \\(\\hat{p}\\) (observed sample proportion) when calculating the standard error and when checking the success/failure condition: $SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$\nFor hypothesis tests use \\(p_0\\) (null value) when calculating the standard error and checking the success/failure condition: $SE_{\\hat{p}} = \\sqrt{\\frac{p_0 (1-p_0)}{n}}$\nSuch a discrepancy doesn’t exist when conducting inference for means, since the mean doesn’t factor into the calculation of the standard error, while the proportion does.\n\nCalculate the required minimum sample size for a given margin of error at a given confidence level, and explain why we use \\(\\hat{p} = 0.5\\) if there are no previous studies suggesting a more accurate estimate.\n\nConceptually: When there is no additional information, 50% chance of success is a good guess for events with only two outcomes (success or failure).\nMathematically: Using \\(\\hat{p} = 0.5\\) yields the most conservative (highest) estimate for the required sample size.\n\nNote that the calculation of the standard error of the distribution of the difference in two independent sample proportions is different for a confidence interval and a hypothesis test.\n\nconfidence interval (and hypothesis test when $H_0: p_1 -p_2 =$ some value other than 0): $SE_{(\\hat{p}_1 - \\hat{p}_2)} = \\sqrt{\\frac{ \\hat{p}_1 (1 - \\hat{p}_1)}{n_1} + \\frac{ \\hat{p}_2 (1 - \\hat{p}_2)}{n_2} }$\nhypothesis test when $H_0: p_1 -p_2 = 0$: $SE_{(\\hat{p}_1 - \\hat{p}_2)} = \\sqrt{\\frac{ \\hat{p}_{pool} (1 - \\hat{p}_{pool})}{n_1} + \\frac{ \\hat{p}_{pool} (1 - \\hat{p}_{pool})}{n_2} }$, where \\(\\hat{p}_{pool}\\) is the overall rate of success: $\\hat{p}_{pool} = \\frac{\\text{number of successes in group 1 + number of successes in group 2}}{n_1 + n_2}$\n\nNote that the reason for the difference in calculations of standard error is the same as in the case of the single proportion: when the null hypothesis claims that the two population proportions are equal, we need to take that into consideration when calculating the standard error for the hypothesis test, and use a common proportion for both samples.\nUse a chi-square test of goodness of fit to evaluate if the distribution of levels of a single categorical variable follows a hypothesized distribution.\n\n\\(H_0:\\) The distribution of the variable follows the hypothesized distribution, and any observed differences are due to chance.\n\\(H_A:\\) The distribution of the variable does not follow the hypothesized distribution.\n\nCalculate the expected counts for a given level (cell) in a one-way table as the sample size times the hypothesized proportion for that level.\nCalculate the chi-square test statistic as $\\chi = \\sum_{i = 1}^{k}  \\frac{(\\text{observed count} - \\text{expected count})^2}{\\text{expected count}}$, where \\(k\\) is the number of cells.\nNote that the chi-square distribution is right skewed with one parameter: degrees of freedom. In the case of a goodness of fit test, \\(df = \\# \\text{of categories} - 1\\).\nList the conditions necessary for performing a chi-square test (goodness of fit or independence)\n\nthe observations should be independent\nexpected counts for each cell should be at least 5\ndegrees of freedom should be at least 2 (if not, use methods for evaluating proportions)\n\nDescribe how to use the chi-square table to obtain a p-value.\nWhen evaluating the independence of two categorical variables where at least one has more than two levels, use a chi-square test of independence.\n\n\\(H_0:\\) The two variables are independent.\n\\(H_A:\\) The two variables are dependent.\n\nCalculate expected counts in two-way tables as [ E = ]\nCalculate the degrees of freedom for chi-square test of independence as \\(df = (R - 1) \\times (C - 1)\\), where \\(R\\) is the number of rows in a two-way table, and \\(C\\) is the number of columns.\nNote that there is no such thing as a chi-square confidence interval for proportions, since in the case of a categorical variables with many levels, there isn’t one parameter to estimate.\nUse simulation methods when sample size conditions aren’t met for inference for categorical variables.\n\nNote that the \\(t\\)-distribution is only appropriate to use for means, when sample size isn’t sufficiently large, and the parameter of interest is a proportion or a difference between two proportions, we need to use simulation.\n\nIn hypothesis testing\n\nfor one categorical variable, generate simulated samples based on the null hypothesis, and then calculate the number of samples that are at least as extreme as the observed data.\nfor two categorical variables, use a randomization test.\n\nUse bootstrap methods for confidence intervals for categorical variables with at most two levels.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "6 - Inference for Categorical Data"
    ]
  },
  {
    "objectID": "chapters/chapter4.html",
    "href": "chapters/chapter4.html",
    "title": "Chapter 4 - Distributions of Random Variables",
    "section": "",
    "text": "Learning Outcomes\n\nDefine the standardized (Z) score of a data point as the number of standard deviations it is away from the mean: \\(Z = \\frac{x - \\mu}{\\sigma}\\).\nUse the Z score\n\nif the distribution is normal: to determine the percentile score of a data point (using technology or normal probability tables)\nregardless of the shape of the distribution: to assess whether or not the particular observation is considered to be unusual (more than 2 standard deviations away from the mean)\n\nDepending on the shape of the distribution determine whether the median would have a negative, positive, or 0 Z score.\nAssess whether or not a distribution is nearly normal using the 68-95-99.7% rule or graphical methods such as a normal probability plot.\n\nReading: Section 4.1 of OpenIntro Statistics\nTest yourself: True/False: In a right skewed distribution the Z score of the median is positive.\n\nIf X is a random variable that takes the value 1 with probability of success \\(p\\) and 0 with probability of success \\(1-p\\), then \\(X\\) is a Bernoulli random variable.\nThe geometric distribution is used to describe how many trials it takes to observe a success.\nDefine the probability of finding the first success in the \\(n^{th}\\) trial as \\((1-p)^{n-1}p\\).\n\n\\(\\mu = \\frac{1}{p}\\)\n\\(\\sigma^2 = \\frac{1-p}{p^2}\\)\n\\(\\sigma = \\sqrt{\\frac{1-p}{p^2}}\\)\n\nDetermine if a random variable is binomial using the four conditions:\n\nThe trials are independent.\nThe number of trials, n, is fixed.\nEach trial outcome can be classified as a success or failure.\nThe probability of a success, p, is the same for each trial.\n\nCalculate the number of possible scenarios for obtaining \\(k\\) successes in \\(n\\) trials using the choose function: \\({n \\choose k} = \\frac{n!}{k!~(n - k)!}\\).\nCalculate probability of a given number of successes in a given number of trials using the binomial distribution: \\(P(k = K) = \\frac{n!}{k!~(n - k)!}~p^k~(1-p)^{(n - k)}\\).\nCalculate the expected number of successes in a given number of binomial trials \\((\\mu = np)\\) and its standard deviation \\((\\sigma = \\sqrt{np(1-p)})\\).\nWhen number of trials is sufficiently large (\\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\)), use normal approximation to calculate binomial probabilities, and explain why this approach works.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "4 - Distributions"
    ]
  },
  {
    "objectID": "chapters/chapter2.html",
    "href": "chapters/chapter2.html",
    "title": "Chapter 2 - Summarizing Data",
    "section": "",
    "text": "Learning Outcomes\n\nUse appropriate visualizations for different types of data (e.g. histogram, barplot, scatterplot, boxplot, etc.).\nUse different measures of center and spread and be able to describe the robustness of different statistics.\nDescribe the shape of distributions vis-a-vis histograms and boxplots.\nCreate and intepret contingency and frequency tables (one- and two-way tables).\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\nggplot2 - ggplot2 is an R package by Wickham that implements the grammer of graphics (Wilkinson, 2005) in R. I will frequently make use of the graphing framework throughout the course and is worth learning.\nVisualizing Likert Data - An R package for visualizing Likert scale data built on the ggplot2 framework.\nQuick-R base graphics - Covers many of the visualizations using R’s base graphics.\n\n\n\nVideos\nSummarizing and Graphing Numerical Data\n\n\nExploring Categorical Data\n\n\n\n\nNote about Pie Charts\nThere is only one pie chart in OpenIntro Statistics (Diez, Barr, & ??etinkaya-Rundel, 2015, p. 48). Consider the following three pie charts that represent the preference of five different colors. Is there a difference between the three pie charts? This is probably a difficult to answer.\n\n\n\nPie\n\n\nHowever, consider the bar plot below. Here, we cleary see there is a difference between the ratio of the three colors. As John Tukey famously said:\n\nThere is no data that can be displayed in a pie chart that cannot better be displayed in some other type of chart\n\n\n\n\nBar\n\n\nSource: https://en.wikipedia.org/wiki/Pie_chart.",
    "crumbs": [
      "Topics",
      "2 - Summarizing Data"
    ]
  },
  {
    "objectID": "chapters/bayesian.html",
    "href": "chapters/bayesian.html",
    "title": "Bayesian Analysis",
    "section": "",
    "text": "Supplemental Readings\n\nChapter 17 of Learning Statistics with R (Navarro, version 0.6)\nFitting a Model by Maximum Likelihood (Collier, 2013).\nKruschke’s website for Doing Bayesian Data Analysis\nKruschke’s blog\nAndrew Gelman’s blog - Posts about bayesian statistics\n\n\n\nVideos\n\nRasmus Bååth’s Introduction to Bayesian Data Analysis Video Series\n\n\n\n\n\n\n\n\nJohn Kruschke’s Video Series\nBayesian Methods Interpret Data Better\n\n\nBayesian Estimation Supersedes the t Test\n\n\nPrecision is the goal",
    "crumbs": [
      "Topics",
      "Bayesian"
    ]
  },
  {
    "objectID": "assignments/labs.html",
    "href": "assignments/labs.html",
    "title": "Labs",
    "section": "",
    "text": "Recommended Homework Questions to Complete for Practice. These will not be graded. The answers are at the end of the textbook. Successfully completing these will help you prepare for the midterm and final exams.\nChapter 1: 1.09 (p. 20), 1.13 (p. 29), 1.27 (p. 31), 1.33 (p. 35)\nChapter 2: 2.33 (p. 78), 2.09 (p. 57), 2.15 (p. 59), 2.25 (p. 76)\nChapter 3: 3.5 (p. 92), 3.7 (p. 93), 3.17 (p. 111), 3.25 (p. 114), 3.33 (p. 124), 3.37 (p, 128)\nChapter 4: 4.1 (p. 142), 4.4 (p. 142), 4.14 (p. 148), 4.29 (p.162)\nChapter 5: 5.3 (179), 5.5 (180), 5.7 (p. 187), 5.9 (p. 187) 5.11 (p. 188), 5.19 (p. 202), 5.25 (p. 203), 5.31 (p. 204), 5.35 (p. 205), 5.37 (p. 205)\nChapter 6: 6.47 (p, 247), 6.09 (p. 216), 6.15 (p. 216), 6.21 (p. 226), 6.33 (p. 239), 6.49 (p. 248)\nChapter 7: 5.23 (p. 203), 7.13, p. 261), 7.19 (p, 266), 7.27 (p.276), 7.33 (p. 284)\nChapter 8: 8.21 (p. 326), 8.13 (p. 316), 8.23 (p. 326), 8.25 (p. 327), 8.43 (p, 340)\nChapter 9: 9.1 (p. 350), 9.3 (p. 352), 9.7 (p, 357), 9.15 (p. 380), 9.17 (p, 381)"
  },
  {
    "objectID": "assignments/daacs.html",
    "href": "assignments/daacs.html",
    "title": "DAACS",
    "section": "",
    "text": "Formative assessments are a type of assessment that help students and teachers monitor student learning. These are low stakes assessments (i.e. you get credit for doing them, not based upon how you did). I will use the results to inform my instruction.\n\n\nComplete this Google Form. The purpose of this survey is for us to get to know you better. There is no passing or failing, right or wrong answers. Please answer each section honestly. We will use the results in aggregate to inform the instruction in this class.\n\n\n\n\nThe Diagnostic Assessment and Achievement of College Skills (DAACS) is a formative assessment designed to provide you with information about key college skills. DAACS includes assessments in self-regulated learning, mathematics, reading, and writing. YOU ARE ONLY REQUIRED TO COMPLETE THE SELF-REGULATED LEARNING ASSESSMENT. This should take about 10 minutes to complete the assessment and there is no passing or failing. Once you are done, we encourage you to review the resources recommended to you. We will use the aggregated results in class. To get credit for this assignment:\n\nGo to cuny.daacs.net\nLogin using your CUNY account.\nComplete the Self-Regulated Learning (SRL) assessment.\n\nDownload the PDF once complete and upload it to Brightspace."
  },
  {
    "objectID": "assignments/daacs.html#formative-assessments",
    "href": "assignments/daacs.html#formative-assessments",
    "title": "DAACS",
    "section": "",
    "text": "Formative assessments are a type of assessment that help students and teachers monitor student learning. These are low stakes assessments (i.e. you get credit for doing them, not based upon how you did). I will use the results to inform my instruction.\n\n\nComplete this Google Form. The purpose of this survey is for us to get to know you better. There is no passing or failing, right or wrong answers. Please answer each section honestly. We will use the results in aggregate to inform the instruction in this class.\n\n\n\n\nThe Diagnostic Assessment and Achievement of College Skills (DAACS) is a formative assessment designed to provide you with information about key college skills. DAACS includes assessments in self-regulated learning, mathematics, reading, and writing. YOU ARE ONLY REQUIRED TO COMPLETE THE SELF-REGULATED LEARNING ASSESSMENT. This should take about 10 minutes to complete the assessment and there is no passing or failing. Once you are done, we encourage you to review the resources recommended to you. We will use the aggregated results in class. To get credit for this assignment:\n\nGo to cuny.daacs.net\nLogin using your CUNY account.\nComplete the Self-Regulated Learning (SRL) assessment.\n\nDownload the PDF once complete and upload it to Brightspace."
  },
  {
    "objectID": "announcements.html",
    "href": "announcements.html",
    "title": "Announcements",
    "section": "",
    "text": "Be sure to check this page regularly for updates. In addition to periodic updates about the course, slides and recordings of the weekly meetups will be posted here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample size and statistical significance for chi-squared tests\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow many times do I need to take a test to randomly get all questions correct?\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to DAV 5300!\n\n\n\n\n\nImportant information on how to get started with this course. Please read this post carefully.\n\n\n\n\n\nJan 17, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Announcements"
    ]
  },
  {
    "objectID": "assignments/exams.html",
    "href": "assignments/exams.html",
    "title": "Exam",
    "section": "",
    "text": "There will be a final exam on the last class. See the course schedule for details."
  },
  {
    "objectID": "assignments/participation.html",
    "href": "assignments/participation.html",
    "title": "Participation",
    "section": "",
    "text": "One Minute Papers\nA “one minute paper” (Angelo & Cross, 1993) is a short written reflection to be completed after each class meetup. You are to answer two questions: 1) What was the most important thing you learned during this class? and 2) What important question remains unanswered for you? Our goal is to give you a moment to reflect on the most important concepts presented were and to provide me with information about what concepts are still unclear. At the completion of each meetup (whether attended live or after watching the recording), complete the Google Form linked from the last slide."
  },
  {
    "objectID": "chapters/chapter1.html",
    "href": "chapters/chapter1.html",
    "title": "Chapter 1 - Introduction to Data",
    "section": "",
    "text": "Learning Objectives\n\nIdentify the type of variables (e.g. numerical or categorical; discrete or continuous; ordered or not ordered).\nIdentify the relationship between multiple variables (i.e. independent vs. dependent).\nDefine variables that are not associated as independent.\nBe able to describe and identify the difference between observational and experimental studies.\nDistinguish between simple random, stratified, and cluster sampling, and recognize the benefits and drawbacks of choosing one sampling scheme over another.\nIdentify the four principles of experimental design and recognize their purposes: control any possible con- founders, randomize into treatment and control groups, replicate by using a sufficiently large sample or repeating the experiment, and block any variables that might influence the response.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos\nOpenIntro provides a number of videos. You may find these helpful while reading the chapter.\nCase Study: Using Stents to Prevent Strokes\n\n\nData Basics: Observations, Variable, and Data Matrices\n\n\nData Collection Principles\n\n\nObservational Studies and Sampling Strategies\n\n\nDesigning Experiments\n\n\nUsing Randomization to Analyze a Gender Discrimination Study",
    "crumbs": [
      "Topics",
      "1 - Intro to Data"
    ]
  },
  {
    "objectID": "chapters/chapter3.html",
    "href": "chapters/chapter3.html",
    "title": "Chapter 3 - Probability",
    "section": "",
    "text": "Learning Outcomes\n\nDefine trial, outcome, and sample space.\nDefine and describe the law of large numbers.\nDistinguish disjoint (also called mutually exclusive) and independent events.\nUse Venn diagrams to represent events and their probabilities.\nDescribe probability distributions.\nDistinguish between marginal and conditional probabilities.\nUse tree diagrams and/or Bayes Theorem to calculate conditional probabilities and probabilities of intersection of non-independent events.\nThe expected value of a discrete random variable is computed by adding each outcome weighted by its probability.\n$$ E(X)=\\mu=\\sum_{i=1}^{k}{{x}_{i}P\\left(X={x}_{i}\\right)} $$\nThe variance of a discrete random variable is computed by adding each squared deviation of an outcome from the expected value weighted by its probability. The standard deviation is the square root of the variance.\n$$ Var(X)={\\sigma}^{2}=\\sum_{i=1}^{k}{{\\left({x}_{i}-\\mu\\right)}P\\left(X={x}_{i}\\right) } $$\nThe average of a linear combination of discrete random variables is computed as the sum of their averages, weighted by the constant multipliers.\nThe variance of a linear combination of independent discrete random variables is computed as the sum of their variances, weighted by the square of the constant multipliers.\nThe distribution of a continuous random variable is described by the probability density function.\nThe total area under the density curve is 1.\nProbabilities under the density curve can be calculated as the area under the curve.\nThe probability of a continuous random variable being exactly equal to a value is 0, since there is no area under the curve at a given location.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\nMatloff, N. (2009). From Algorithms to Z-Scores: Probabilistic and Statistical Modeling in Computer Science. Available from http://heather.cs.ucdavis.edu/probstatbook.\n\n\n\nVideos\nProbability Introduction\n\n\nWould You Take This Bet?\n\n\nThe Monty Hall Problem",
    "crumbs": [
      "Topics",
      "3 - Probability"
    ]
  },
  {
    "objectID": "chapters/chapter5.html",
    "href": "chapters/chapter5.html",
    "title": "Chapter 5 - Foundations for Inference",
    "section": "",
    "text": "Learning Outcomes\n\nDefine sample statistic as a point estimate for a population parameter, for example, the sample proportion is used to estimate the population proportion, and note that point estimate and sample statistic are synonymous.\nRecognize that point estimates (such as the sample proportion) will vary from one sample to another, and define this variability as sampling variation.\nCalculate the sampling variability of the proportion, the standard error, as \\(SE = \\sqrt{\\frac{p(1-p)}{n}}\\), where \\(p\\) is the population proportion.\n\nNote that when the population proportion \\(p\\) is not known (almost always), this can be estimated using the sample proportion, \\(SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).\n\nStandard error measures the variability in point estimates from different samples of the same size and from the same population, i.e. measures the sampling variability.\nRecognize that when the sample size increases we would expect the sampling variability to decrease.\n\nConceptually: Imagine taking many samples from the population. When sample sizes are large the sample proportion will be much more consistent across samples than when the sample sizes are small.\nMathematically: \\(SE = ???\\), when \\(n\\) increases, \\(SE\\) will decrease since \\(n\\) is in the denominator.\n\nNotice that sampling distributions of point estimates coming from samples that don’t meet the required conditions for the CLT (about sample size and independence) will not be normal.\nDefine a confidence interval as the plausible range of values for a population parameter.\nDefine the confidence level as the percentage of random samples which yield confidence intervals that capture the true population parameter.\nCalculate an approximate 95% confidence interval by adding and subtracting 2 standard errors to the point estimate: \\(point~estimate \\pm 2 \\times SE\\).\nRecognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.\n\nIn the case of the proportion the CLT tells us that if\n\nthe observations in the sample are independent, and\nthere are at least 10 successes and 10 failures, \\end{itemize} then the distribution of the sample proportion will be nearly normal, centered at the true population proportion and with a standard error of \\(\\sqrt{\\frac{p(1-p)}{n}}\\). [ N ( mean = p, SE = ) ]\n\nWhen the population proportion is unknown, condition (2) can be checked using the sample proportion.\n\nRecall that independence of observations in a sample is provided by random sampling (in the case of observational studies) or random assignment (in the case of experiments).\n\nIn addition, the sample should not be large compared to the population, or more precisely, should be smaller than 10% of the population, since samples that are too large will likely contain observations that are not independent. \\end{itemize}\n\nRecognize that the nearly normal distribution of the point estimate (as suggested by the CLT) implies that a more precise confidence interval can be calculated as [ point~estimate z^{} SE, ] where \\(z^{\\star}\\) corresponds to the cutoff points in the standard normal distribution to capture the middle XX% of the data, where XX% is the desired confidence level.\n\nFor proportions this is \\(\\bar{x} \\pm Z^\\star \\sqrt{\\frac{p(1-p)}{n}}\\).\nNote that \\(z^{\\star}\\) is always positive.\n\nDefine margin of error as the distance required to travel in either direction away from the point estimate when constructing a confidence interval, i.e. \\(z^{\\star} \\times SE\\).\n\nNotice that this corresponds to half the width of the confidence interval.\n\nInterpret a confidence interval as “We are XX% confident that the true population parameter is in this interval”, where XX% is the desired confidence level.\n\nNote that your interpretation must always be in context of the data – mention what the population is and what the parameter is (mean or proportion).\n\nExplain how the hypothesis testing framework resembles a court trial.\nRecognize that in hypothesis testing we evaluate two competing claims:\n\nthe null hypothesis, which represents a skeptical perspective or the status quo, and\nthe alternative hypothesis, which represents an alternative under consideration and is often represented by a range of possible parameter values.\n\nConstruction of hypotheses:\n\nAlways construct hypotheses about population parameters (e.g. population proportion, \\(p\\)) and not the sample statistics (e.g. sample proportion, \\(\\hat{p}\\)). Note that the population parameter is unknown while the sample statistic is measured using the observed data and hence there is no point in hypothesizing about it.\nDefine the null value as the value the parameter is set to equal in the null hypothesis.\nNote that the alternative hypothesis might be one-sided (\\(\\mu\\) \\(&lt;\\) or \\(&gt;\\) the null value) or two-sided (\\(\\mu\\) \\(\\ne\\) the null value), and the choice depends on the research question.\n\nDefine a p-value as the conditional probability of obtaining a sample statistic at least as extreme as the one observed given that the null hypothesis is true. $\\text{p-value} = \\text{P(observed or more extreme sample statistic}~|~H_0 \\text{ true)}$\nCalculate a p-value as the area under the normal curve beyond the observed sample proportion (either in one tail or both, depending on the alternative hypothesis). Note that in doing so you can use a Z score, where $Z = \\frac{sample~statistic - null~value}{SE} = \\frac{\\bar{x} - \\mu_0}{SE}$\n\nAlways sketch the normal curve when calculating the p-value, and shade the appropriate area(s) depending on whether the alternative hypothesis is one- or two-sided.\n\nInfer that if a confidence interval does not contain the null value the null hypothesis should be rejected in favor of the alternative.\nCompare the p-value to the significance level to make a decision between the hypotheses:\n\nIf the p-value \\(&lt;\\) the significance level, reject the null hypothesis since this means that obtaining a sample statistics at least as extreme as the observed data is extremely unlikely to happen just by chance, and conclude that the data provides evidence for the alternative hypothesis.\nIf the p-value \\(&gt;\\) the significance level, fail to reject the null hypothesis since this means that obtaining a sample statistics at least as extreme as the observed data is quite likely to happen by chance, and conclude that the data does not provide evidence for the alternative hypothesis.\nNote that we can never “accept” the null hypothesis since the hypothesis testing framework does not allow us to confirm it.\n\nNote that the conclusion of a hypothesis test might be erroneous regardless of the decision we make.\n\nDefine a Type 1 error as rejecting the null hypothesis when the null hypothesis is actually true.\nDefine a Type 2 error as failing to reject the null hypothesis when the alternative hypothesis is actually true.\n\nChoose a significance level depending on the risks associated with Type 1 and Type 2 errors.\n\nUse a smaller \\(\\alpha\\) is Type 1 error is relatively riskier.\nUse a larger \\(\\alpha\\) is Type 2 error is relatively riskier.\n\nFormulate the framework for statistical inference using hypothesis testing and nearly normal point estimates:\n\nSet up the hypotheses first in plain language and then using appropriate notation.\nIdentify the appropriate sample statistic that can be used as a point estimate for the parameter of interest.\nVerify that the conditions for the CLT holds.\nCompute the SE, sketch the sampling distribution, and shade area(s) representing the p-value.\nUsing the sketch and the normal model, calculate the p-value and determine if the null hypothesis should be rejected or not, and state your conclusion in context of the data and the research question.\n\nIf the conditions necessary for the CLT to hold are not met, note this and do not go forward with the analysis. (We will later learn about methods to use in these situations.)\nDistinguish statistical significance vs. practical significance.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\nWhy do we use 0.05 as a significance level?\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "5 - Foundations for Inference"
    ]
  },
  {
    "objectID": "chapters/chapter7.html",
    "href": "chapters/chapter7.html",
    "title": "Chapter 7 - Inference for Numerical Data",
    "section": "",
    "text": "Learning Outcomes\n\nUse the \\(t\\)-distribution for inference on a single mean, difference of paired (dependent) means, and difference of independent means.\nExplain why the \\(t\\)-distribution helps make up for the additional variability introduced by using \\(s\\) (sample standard deviation) in calculation of the standard error, in place of \\(\\sigma\\) (population standard deviation).\nDescribe how the \\(t\\)-distribution is different from the normal distribution, and what ?heavy tail? means in this context.\nNote that the \\(t\\)-distribution has a single parameter, degrees of freedom, and as the degrees of freedom increases this distribution approaches the normal distribution.\nUse a \\(t\\)-statistic, with degrees of freedom \\(df = n - 1\\) for inference for a population mean:\n\nStandard error: \\(SE = \\frac{s}{\\sqrt{n}}\\)\nConfidence interval: $\\bar{x} \\pm t_{df}^\\star SE$\nHypothesis test: $T_{df} = \\frac{\\bar{x} - \\mu}{SE}$ \\end{itemize}\n\nDescribe how to obtain a p-value for a \\(t\\)-test and a critical \\(t\\)-score ($t^\\star_{df}$) for a confidence interval.\nDefine observations as paired if each observation in one dataset has a special correspondence or connection with exactly one observation in the other data set.\nCarry out inference for paired data by first subtracting the paired observations from each other, and then treating the set of differences as a new numerical variable on which to do inference (such as a confidence interval or hypothesis test for the average difference).\nCalculate the standard error of the difference between means of two paired (dependent) samples as $SE = \\frac{s_{diff}}{\\sqrt{n_{diff}}}$ and use this standard error in hypothesis testing and confidence intervals comparing means of paired (dependent) groups.\nUse a \\(t\\)-statistic, with degrees of freedom $df = n_{diff} - 1$ for inference for a population mean: \\begin{itemize}\n\nStandard error: \\(SE = \\frac{s}{\\sqrt{n}}\\)\nConfidence interval: $\\bar{x}_{diff} \\pm t_{df}^\\star SE$\nHypothesis test: $T_{df} = \\frac{\\bar{x}_{diff} - \\mu_{diff}}{SE}$. Note that $\\mu_{diff}$ is often 0, since often $H_0: \\mu_{diff} = 0$.\n\nRecognize that a good interpretation of a confidence interval for the difference between two parameters includes a comparative statement (mentioning which group has the larger parameter).\nRecognize that a confidence interval for the difference between two parameters that doesn?t include 0 is in agreement with a hypothesis test where the null hypothesis that sets the two parameters equal to each other is rejected.\nCalculate the standard error of the difference between means of two independent samples as $SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$, and use this standard error in hypothesis testing and confidence intervals comparing means of independent groups.\nUse a \\(t\\)-statistic, with degrees of freedom $df = min(n_1 - 1, n_2 - 1)$ for inference for a population mean:\n\nStandard error: $\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$\nConfidence interval: $(\\bar{x}_1 - \\bar{x}_2) \\pm t_{df}^\\star SE$\nHypothesis test: $T_{df} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{SE}$. Note that \\(\\mu_{diff}\\) is often 0, since often $H_0: \\mu_1 - \\mu_2 = 0$.\n\nCalculate the power of a test for a given effect size and significance level in two steps: (1) Find the cutoff for the sample statistic that will allow the null hypothesis to be rejected at the given significance level, (2) Calculate the probability of obtaining that sample statistic given the effect size.\nExplain how power changes for changes in effect size, sample size, significance level, and standard error.\nDefine analysis of variance (ANOVA) as a statistical inference method that is used to determine if the variability in the sample means is so large that it seems unlikely to be from chance alone by simultaneously considering many groups at once.\nRecognize that the null hypothesis in ANOVA sets all means equal to each other, and the alternative hypothesis suggest that at least one mean is different. \\begin{itemize}\n\n$H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k$\n\\(H_A:\\) At least one mean is different\n\nList the conditions necessary for performing ANOVA\n\nthe observations should be independent within and across groups\nthe data within each group are nearly normal\nthe variability across the groups is about equal\nand check if they are met using graphical diagnostics.\n\nRecognize that the test statistic for ANOVA, the F statistic, is calculated as the ratio of the mean square between groups (MSG, variability between groups) and mean square error (MSE, variability within errors), and has two degrees of freedom, one for the numerator ($df_{G} = k - 1$, where \\(k\\) is the number of groups) and one for the denominator ($df_{E} = n - k$, where \\(n\\) is the total sample size).\n\nNote that you won’t be expected to calculate MSG or MSE from the raw data, but you should have a conceptual understanding of how they’re calculated and what they measure.\n\nDescribe why calculation of the p-value for ANOVA is always “one sided”.\nDescribe why conducting many \\(t\\)-tests for differences between each pair of means leads to an increased Type 1 Error rate, and we use a corrected significance level (Bonferroni corection, \\(\\alpha^\\star = \\alpha / K\\), where \\(K\\) is the e number of comparisons being considered) to combat inflating this error rate.\nDescribe why it is possible to reject the null hypothesis in ANOVA but not find significant differences between groups as a result of pairwise comparisons.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "7 - Inference for Numerical Data"
    ]
  },
  {
    "objectID": "chapters/chapter9.html",
    "href": "chapters/chapter9.html",
    "title": "Chapter 9 - Multiple and Logistic Regression",
    "section": "",
    "text": "Learning Outcomes\n\nDefine the multiple linear regression model as $$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k$$ where there are \\(k\\) predictors (explanatory variables).\nInterpret the estimate for the intercept (\\(b_0\\)) as the expected value of \\(y\\) when all predictors are equal to 0, on average.\nInterpret the estimate for a slope (say \\(b_1\\)) as “All else held constant, for each unit increase in \\(x_1\\), we would expect \\(y\\) to increase/decrease on average by \\(b_1\\).”\nDefine collinearity as a high correlation between two independent variables such that the two variables contribute redundant information to the model – which is something we want to avoid in multiple linear regression.\nNote that \\(R^2\\) will increase with each explanatory variable added to the model, regardless of whether or not the added variables is a meaningful predictor of the response variable. Therefore we use adjusted \\(R^2\\), which applies a penalty for the number of predictors included in the model, to better assess the strength of a multiple linear regression model: $$R^2 = 1 - \\frac{Var(e_i) / (n - k - 1)}{Var(y_i) / (n - 1)}$$ where $Var(e_i)$ measures the variability of residuals ($SS_{Err}$), $Var(y_i)$ measures the total variability in observed \\(y\\) ($SS_{Tot}$), \\(n\\) is the number of cases and \\(k\\) is the number of predictors.\n\nNote that adjusted \\(R^2\\) will only increase if the added variable has a meaningful contribution to the amount of explained variability in \\(y\\), i.e. if the gains from adding the variable exceeds the penalty.\n\nDefine model selection as identifying the best model for predicting a given response variable.\nNote that we usually prefer simpler (parsimonious) models over more complicated ones.\nDefine the full model as the model with all explanatory variables included as predictors.\nNote that the p-values associated with each predictor are conditional on other variables being included in the model, so they can be used to assess if a given predictor is significant, given that all others are in the model.\n\nThese p-values are calculated based on a \\(t\\) distribution with \\(n - k - 1\\) degrees of freedom.\nThe same degrees of freedom can be used to construct a confidence interval for the slope parameter of each predictor: $$b_i \\pm t^\\star_{n - k - 1} SE_{b_i}$$\n\nStepwise model selection (backward or forward) can be done based based on adjusted \\(R^2\\) (choose the model with higher adjusted \\(R^2\\)).\nThe general idea behind backward-selection is to start with the full model and eliminate one variable at a time until the ideal model is reached.\n\nStart with the full model.\nRefit all possible models omitting one variable at a time, and choose the model with the highest adjusted \\(R^2\\).\nRepeat until maximum possible adjusted \\(R^2\\) is reached.\n\nThe general idea behind forward-selection is to start with only one variable and adding one variable at a time until the ideal model is reached.\n\nTry all possible simple linear regression models predicting \\(y\\) using one explanatory variable at a time. Choose the model with the highest adjusted \\(R^2\\).\nTry all possible models adding one more explanatory variable at a time, and choose the model with the highest adjusted \\(R^2\\).\nRepeat until maximum possible adjusted \\(R^2\\) is reached.\n\nAdjusted \\(R^2\\) method is more computationally intensive, but it is more reliable, since it doesn’t depend on an arbitrary significant level.\nList the conditions for multiple linear regression as\n\nlinear relationship between each (numerical) explanatory variable and the response - checked using scatterplots of \\(y\\) vs. each \\(x\\), and residuals plots of \\(residuals\\) vs. each \\(x\\)\nnearly normal residuals with mean 0 - checked using a normal probability plot and histogram of residuals\nconstant variability of residuals - checked using residuals plots of \\(residuals\\) vs. \\(\\hat{y}\\), and \\(residuals\\) vs. each \\(x\\)\nindependence of residuals (and hence observations) - checked using a scatterplot of \\(residuals\\) vs. order of data collection (will reveal non-independence if data have time series structure)\n\nNote that no model is perfect, but even imperfect models can be useful.\n\n\n\nSupplemental Readings\n\nOpenIntro Statistics slides\n\n\n\nVideos",
    "crumbs": [
      "Topics",
      "9 - Multiple and Logistic Regression"
    ]
  },
  {
    "objectID": "course/textbooks.html",
    "href": "course/textbooks.html",
    "title": "Textbooks",
    "section": "",
    "text": "Required\n\nDiez, D.M., Barr, C.D., & Çetinkaya-Rundel, M. (2019). OpenIntro Statistics (4th Ed).\n\nThis is an open source textbook and can be downloaded in PDF format here, from the OpenIntro website, or a printed copy can be ordered from Amazon.\n\n \nNavarro, D. (2018, version 0.6). Learning Statistics with R\n\nThis is free textbook that supplements a lot of the material covered in Diez and Barr. We will use the chapter on Bayesian analysis. You can download a PDF version, Bookdown version, or visit the author’s website at learningstatisticswithr.com.\n\n\n\nRecommended\nWickham, H., & Grolemund, G. (2016) R for Data Science. O’Reilly.\n\nMost of this books is available freely online at r4ds.had.co.nz/ but can be purchased from Amazon.\n\nWickham, H. Advanced R. Baca Raton, FL: Taylor & Francis Group.\n\nMost of this book is available freely online at adv-r.had.co.nz but can be purchased from Amazon.\n\nKruschke, J.K. (2014). Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan (2nd Ed). London: Academic Press.\n\nThis book can be purchased from Amazon, but also check out the author’s webiste (doingbayesiandataanalysis.blogspot.com/) for additional resources.",
    "crumbs": [
      "Course information",
      "Textbooks"
    ]
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Project",
    "section": "",
    "text": "The purpose of the data project is for you to conduct a reproducible analysis with a data set of your choosing. There are two components to the project, the proposal, which will be graded on a pass/fail basis, and the final report. The outline for each of these are provided in the templates. When submitting the assignments, include the R Markdown file (change the name to include your last name, for example Bryer-Proposal.Rmd and Bryer-Project.Rmd) along with any supplementary files necessary to run the R Markdown file (e.g. data files, screenshots, etc.). Suggestions for possible data sources are included below, however you are free to use data not listed below. The only requirement is that you are allowed to share the data. Projects will be shared with others on this website so should be presented in a way that other students can reproduce your analysis.",
    "crumbs": [
      "Course information",
      "Data Project"
    ]
  },
  {
    "objectID": "assignments/project.html#project-proposal",
    "href": "assignments/project.html#project-proposal",
    "title": "Project",
    "section": "Project Proposal",
    "text": "Project Proposal\nThe proposal can be more informal using bullet points where necessary and include R code and output. You must address the following areas:\n\nResearch question\nWhat are the cases, and how many are there?\nDescribe the method of data collection.\nWhat type of study is this (observational/experiment)?\nData Source: If you collected the data, state self-collected. If not, provide a citation/link.\nResponse: What is the response variable, and what type is it (numerical/categorical)?\nExplanatory: What is the explanatory variable(s), and what type is it (numerical/categorical)?\nRelevant summary statistics",
    "crumbs": [
      "Course information",
      "Data Project"
    ]
  },
  {
    "objectID": "assignments/project.html#final-project",
    "href": "assignments/project.html#final-project",
    "title": "Project",
    "section": "Final Project",
    "text": "Final Project\nPlease see the course schedule for when project proposals and presentations will be due.\n\nChecklist / Suggested Outline\n\nAbstract (no more than 300 words)\nOverview slide\n\nContext on the data collection\nDescription of the dependent variable (what is being measured)\nDescription of the independent variable (what is being measured; include at least 2 variables)\nResearch question\n\nSummary statistics\nInclude appropriate data visualizations.\nStatistical output\n\nInclude the appropriate statistics for your method used.\nFor null hypothesis tests (e.g. t-test, chi-squared, ANOVA, etc.), state the null and alternative hypotheses along with relevant statistic and p-value (and confidence interval if appropriate).\nFor regression models, include the regression output and interpret the R-squared value.\n\nConclusion\n\nWhy is this analysis important?\nLimitations of the analysis?\n\n\n\n\nRubric\n\n\n\n\n\n\n\n\n\nDomain\nAccomplished\nProficient\nNeeds Improvement\n\n\n\n\nAbstract\nAbstract is less than 300 words, free of grammatical errors, summarizes the analysis conducted, has a conclusion and implicaitons\nNA\nNA\n\n\nIntroduction\nThe research question is clearly stated, can be answered by the data, and the context of the problem clearly explained.\nThe research question is unclear and/or not supported by the data.\nResearch question is ambiguous, unclear, or not stated.\n\n\nData Display\nIncludes appropriate, well-labeled, accurate displays (graphs and tables) of the data.\nIncludes appropriate, accurate displays of the data.\nIncludes appropriate but no accurate displays of the data.\n\n\nData Analysis\nThe appropriate statistical test(s) was used for the data and interpretation was clear.\nThe appropriate statistical test(s) was used but interpretation was not fully clear or well articulated.\nThe incorrect statistical test was used an/or not justified for the data as presented.\n\n\nConclusion\nConclusion includes a clear answer to the statistical question that is consistent with the data analysis and the method of data collection.\nConclusion includes an answer to the statistical question that is consistent with the data but not with the data collection method.\nConclusion does not include an answer to the statistical question that is consistent with the data analysis.\n\n\nOverall Presentation\nAttractive, well-organized, well-written presentation\nPresentation has two of the three qualities: attractive, well-organized, well-written.\nPresentation is not attractive, organized, or written. There are numerous errors throughout.",
    "crumbs": [
      "Course information",
      "Data Project"
    ]
  },
  {
    "objectID": "assignments/project.html#example-data-sources",
    "href": "assignments/project.html#example-data-sources",
    "title": "Project",
    "section": "Example Data Sources",
    "text": "Example Data Sources\nYou are not to use data sources used in class or the textbooks. Possible data sources include, but are not limited to:\n\nFiveThirtyEight https://github.com/fivethirtyeight/data\nRStudio data sources http://blog.rstudio.org/2014/07/23/new-data-packages/\nAnalyze Survey Data for Free (ASDFree) has many open data sources that can be used http://www.asdfree.com/\nThe World Bank Data Catalog http://datacatalog.worldbank.org/\nGoogle Public Data search engine http://www.google.com/publicdata/directory\nVanderbilt data sources http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\nProgramme of International Student Assessment (PISA) http://www.oecd.org/pisa/\nBehavioral Risk Factor Surveillance System (BRFSS) http://www.cdc.gov/brfss/\nWorld Values Survey http://www.worldvaluessurvey.org/wvs.jsp\nAmerican National Election Survey (ANES) http://www.electionstudies.org/\nGeneral Social Survey (GSS) http://www3.norc.org/GSS+Website/\nIntegrated Postsecondary Education Data System (IPEDS) https://nces.ed.gov/ipeds/\nU.S. Census and American Community Survey https://cran.r-project.org/web/packages/acs/index.html\n10 Standard Datasets for Practicing Applied Machine Learning\nAwesome Public Datasets\nUCI Machine Learning Repository - See also this R package: https://github.com/tyluRp/ucimlr\nOpenML",
    "crumbs": [
      "Course information",
      "Data Project"
    ]
  },
  {
    "objectID": "course/syllabus.html",
    "href": "course/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: Jason Bryer, Ph.D.\nClass Meetup: Tuesdays (5:30pm to 7:30pm or 7:45pm to 9:45pm)\nOffice Hours: By appointment\nEmail: jason.bryer@yu.edu\n\nCourse Description\nDeeper math literacy and computational thinking are essential for deeper data literacy. Probability, statistics, and mathematics—especially fundamental linear algebra—are critical to the success of data analysts as they implement increasingly complex solutions. This course is designed to give the non-mathematician practice using mathematical and statistical computational methods in the service of data analytic solutions.\n\n\nAttendance\nYou are expected to attend class to learn the material for the course. Attendance will be recorded for each session. While one absence will not be a problem, several absences will be. That said, if your circumstances warrant missing class, please discuss with me as soon as possible.\n\n\nCourse Learning Outcomes:\nBy then end of the course, students should be able to:\n\nUnderstand the foundations of probability theory and perform basic probability calculations.\nModel situations involving uncertainty using appropriate probability distributions and conditional techniques.\nExplore and summarize data using descriptive statistics.\nTest hypotheses using classical and modern computational techniques.\nConstruct estimators and calculate intervals using classical and modern computational techniques.\nPerform basic Bayesian statistical techniques for estimation and testing hypotheses.\n\n\n\nAssignments and Grading\nYou will be assessed using a variety of methods, namely:\nLabs (30%) - Labs are designed to provide you an opportunity to apply statistical concepts using statistical software.\nTextbook questions (15%) - The assigned questions from the textbook provide an opportunity to assess conceptional understandings. Questions are from the Exercises section at the end of each chapter.\nParticipation (10%) - You are expected to attend every class and to complete a one minute paper at the conclusion of class.\nData Project (25%) - In a group of 2 to 3 students will present the results of analysis using a data set of your choice. More details will be provided a few weeks into the class.\nFinal exam (20%) - A multiple choice exam will be given on the last day of class.\nAll assignments are due on Tuesday Assignments submitted late will be penalized. Assignments will not be accepted more than one week after their due date.\nThe following grading scale will be used for this class.\n\n\n\n\n\n\n\n\n\nQuality of Performance\nLetter\nGrade Range (%)\nGPA/Quality Points\n\n\n\n\nExcellent - work is of exceptional quality\nA.\n93 – 100.\n4.0\n\n\n\nA.\n90 – 92.9.\n3.7\n\n\nGood - work is above average\nB+.\n87 – 89.9.\n3.3\n\n\nSatisfactory\nB\n83 – 86.9\n3.0\n\n\nBelow Average\nB-\n80 – 82.9\n2.7\n\n\nPoor\nC+\n77 – 79.9\n2.3\n\n\n\nC\n70 – 76.9\n2.0\n\n\nFailure\nF\n&lt; 70\n0.0\n\n\n\n\n\nUniversity Policies and Resources\n\nAccessibility and Accomodations\nThe Office of Disability Services collaborates with students, faculty, and staff to provide reasonable accommodations and services to students with disabilities. Students with disabilities who are enrolled in this course and who will be requesting documented disability-related accommodations should make an appointment with the Office of Disability Services, during the first week of class. Once you have been approved for accommodations, please submit your accommodation letter to ensure the successful implementation of those accommodations. For more information, please visit http://yu.edu/Student-Life/Resources-and-Services/Disability-Services/Links to an external site.\n\n\nAcademic Integrity\nThe submission by a student of any examination, course assignment, or degree requirement is assumed to guarantee that the thoughts and expressions therein not expressly credited to another are literally the student’s own. Evidence to the contrary will result in appropriate penalties. For more information, visit https://www.yu.edu/academic-integrity.\nWith the exception of the data project, I expect you to complete all assignments (e.g. homework, labs) on your own. It is fine to ask questions of your peers and professor, but working together and/or sharing answers is not allowed",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "posts/2024-08-28-Welcome.html",
    "href": "posts/2024-08-28-Welcome.html",
    "title": "Welcome to DAV 5300!",
    "section": "",
    "text": "Welcome to DAV 5300! Couple of important notes as you get started:\n\nThe course syllabus located on this website here: https://spring2025.dav5300.net/course/syllabus.html We will post all course materials there. Canvas will be used primarily for submitting assignments. Please read the syllabus carefully! Send us any questions you may still have about the course.\nComplete this Google Form as soon as possible. We will use some of the data (in aggregate) in the meetup.\nOnce you go through the syllabus, try starting Lab 1 as soon as you can. This will require you to install R and RStudio and will help get you get acquainted with R. See the software page for more information about the software we will use for this course.\n\nI am looking forward to getting to know everyone and a fantastic semester! Good luck!"
  },
  {
    "objectID": "posts/2025-03-04-chi_squared_sample_sizes.html",
    "href": "posts/2025-03-04-chi_squared_sample_sizes.html",
    "title": "Sample size and statistical significance for chi-squared tests",
    "section": "",
    "text": "In this post we are going to explore the relationship between sample size (n) and statistical significance for the chi-squared (\\(\\chi^2\\)) test. Recall that from the normal distribution, we construct a confidence interval using:\n\\[ CI = \\bar{x} \\pm z \\cdot SE\\]\nwhere z is the test statistic and:\n\\[ SE =  \\frac{s}{\\sqrt{n}} \\]\nwhere s is the sample standard deviation. Typically our null is zero in which case we reject the null hypothesis when the confidence does not span zero. If we wish to construct a 95% confidence interval, then \\(z = 1.96\\). Assuming the sample standard deviation is constant regardless of sample size (a fair assumption), then as n increases the standard error decreases. The following calculates the confidence interval for n ranging from 10 to 400 assuming a sample standard deviation of 0.15 and 95% confidence level. When \\(n &gt; 171\\) then \\(p &lt; 0.05\\).\n\n# Define some parameters\nsig_level &lt;- .95  # Significance level, 95% here\nes &lt;- 0.15        # Effect size in standard units\nnull_val &lt;- 0     # null value\n\n#' Calculate the standard error\n#' \n#' This function will calculate the standard error from a vector of observations or with a given\n#' sample standard deviation and sample size.\n#' \n#' @param x numeric vector of observations.\n#' @param sigma the sample standard deviation.\n#' @param n sample size.\nstandard_error &lt;- function(x, sigma = sd(x), n = length(x)) {\n    if(!missing(x)) { # Some basic error checking\n        if(sigma != sd(x)) { warning('The sample standard deviation (sigma) is not equal to sd(x)')}\n        if(n != length(x)) { warning('The sample size (n) is not equal to length(x).' )}\n    }\n    return(sigma / sqrt(n))\n}\n# Create a data.frame with varying sample sizes and the corresponding standard error\ndf &lt;- data.frame(\n    n = 10:400,\n    se = standard_error(sigma = 1, n = 10:400)\n)\ncv &lt;- abs(qnorm((1 - sig_level) / 2)) # Critical value (z test statistic)\ndf$ci_low &lt;- es - cv * df$se\ndf$ci_high &lt;- es + cv * df$se\ndf$sig &lt;- null_val &lt; df$ci_low | null_val &gt; df$ci_high\nmin_n &lt;- df$n[df$sig] |&gt; min()\nggplot(df, aes(x = n, y = se, color = sig)) + \n    geom_path() +\n    geom_point() +\n    scale_color_brewer(paste0('p &lt; ', (1 - sig_level)), type = 'qual', palette = 6) +\n    ggtitle(paste0('Minumum n for p &lt; ', (1 - sig_level), ': ', min_n),\n            subtitle = paste0('effect size: ', es, '; null value: ', null_val))\n\n\n\n\n\n\n\n\nThe chi-squared (\\(\\chi^2\\)) test statistic is defined as:\n\\[ \\chi^2 = \\sum{\\frac{(O - E)^2}{E}} \\]\nwhere O is the observed count and E is the expected count. Unlike the standard error for numerical data, n is not explicitly in the formula and therefore makes it a bit more challenging to determine the impact sample size has rejecting the null hypothesis. Moreover, since the chi-squared is calculated from the cell counts in a table of varying length and dimension (one- or two-dimensions specifically) determining how n impacts rejecting the null or not requires more parameters.\nAnswering the question of how large does n need to be to detect a statistically significant result (i.e. to reject the null hypothesis) is refereed to as power. Whereas for calculating the power for numerical data had one parameter, the sample standard deviation, here we need to consider the proportion of observations within different cells. For example, consider we have a variable with three levels and we expect the proportion of observations in the three groups to be 33%, 25%, and 42%, respectively. If our sample size is 100 then we expect there to be 33, 25, and 42 and observations for the three categories. This function will, for varying sample sizes, calculate the counts for the categories to achieve that sample size, estimate the chi-squared statistic and record the p-value. There are other parameters that are documented below. A plot function is also defined using the S3 objected oriented framework.\n\n#' Calculate p-value from a chi-squared test with varying sample sizes\n#'\n#' This algorithm will start with an initial sample size (`n_start`) and perform a chi-squared test\n#' with a vector of counts equal to `n * probs`. This will repeat increasing the sample size by\n#' `n_step` until the p-value from the chi-squared test is less than `p_stop`.\n#'\n#' @param vector of cell probabilities. The sum of the values must equal 1.\n#' @param sig_level significance level.\n#' @param p_stop the p-value to stop estimating chi-squared tests.\n#' @param max_n maximum n to attempt if `p_value` is never less than `p_stop`.\n#' @param min_cell_size minimum size per cell to perform the chi-square test.\n#' @param n_start the starting sample size.\n#' @param n_step the increment for each iteration.\n#' @return a data.frame with three columns: n (sample size), p_value, and sig (TRUE if\n#'         p_value &lt; sig_level).\n#' @importFrom DescTools power.chisq.test CramerV\nchi_squared_power &lt;- function(\n        probs,\n        sig_level = 0.05,\n        p_stop = 0.01,\n        power = 0.80,\n        power_stop = 0.90,\n        max_n = 100000,\n        min_cell_size = 10,\n        n_start = 10,\n        n_step = 10\n) {\n    if(sum(probs) != 1) { # Make sure the sum is equal to 1\n        stop('The sum of the probabilities must equal 1.')\n    } else if(length(unique(probs)) == 1) {\n        stop('All the probabilities are equal.')\n    }\n\n    n &lt;- n_start\n    p_values &lt;- numeric()\n    power_values &lt;- numeric()\n    df &lt;- ifelse(is.vector(probs),\n                 length(probs) - 1,\n                 min(dim(probs)) - 1) # Degrees of freedom\n    repeat {\n        x &lt;- (probs * n) |&gt; round()\n        if(all(x &gt; min_cell_size)) {\n            cs &lt;- chisq.test(x, rescale.p = TRUE, simulate.p.value = FALSE)\n            p_values[n / n_step] &lt;- cs$p.value\n            pow &lt;- DescTools::power.chisq.test(\n                n = n,\n                w = DescTools::CramerV(as.table(x)),\n                df = df,\n                sig.level = sig_level\n            )\n            power_values[n / n_step] &lt;- pow$power\n            if((cs$p.value &lt; p_stop & pow$power &gt; power_stop) | n &gt; max_n) {\n                break;\n            }\n        } else {\n            p_values[n / n_step] &lt;- NA\n            power_values[n / n_step] &lt;- NA\n        }\n        n &lt;- n + n_step\n    }\n    result &lt;- data.frame(n = seq(10, length(p_values) * n_step, n_step),\n                         p_value = p_values,\n                         sig = p_values &lt; sig_level,\n                         power = power_values)\n    class(result) &lt;- c('chisqpower', 'data.frame')\n    attr(result, 'probs') &lt;- probs\n    attr(result, 'sig_level') &lt;- sig_level\n    attr(result, 'p_stop') &lt;- p_stop\n    attr(result, 'power') &lt;- power\n    attr(result, 'power_stop') &lt;- power_stop\n    attr(result, 'max_n') &lt;- max_n\n    attr(result, 'n_step') &lt;- n_step\n    return(result)\n}\n\n#' Plot the results of chi-squared power estimation\n#'\n#' @param x result of [chi_squared_power()].\n#' @param plot_power whether to plot the power curve.\n#' @param plot_p whether to plot p-values.\n#' @param digits number of digits to round to.\n#' @param segement_color color of the lines marking where power and p values exceed threshold.\n#' @param sgement_linetype linetype of the lines marking where power and p values exceed threshold.\n#' @param p_linetype linetype for the p-values.\n#' @param power_linetype linetype for the power values.\n#' @param title plot title. If missing a title will be automatically generated.\n#' @parma ... currently not used.\n#' @return a ggplot2 expression.\nplot.chisqpower &lt;- function(\n        x,\n        plot_power = TRUE,\n        plot_p = TRUE,\n        digits = 4,\n        segment_color = 'grey60',\n        segment_linetype = 1,\n        p_linetype = 1,\n        power_linetype = 2,\n        title,\n        ...\n) {\n    pow &lt;- attr(x, 'power')\n\n    p &lt;- ggplot(x[!is.na(x$p_value),], aes(x = n, y = p_value))\n\n    if(plot_power) {\n        if(any(x$power &gt; pow, na.rm = TRUE)) {\n            min_n_power &lt;- min(x[x$power &gt; pow,]$n, na.rm = TRUE)\n            p &lt;- p +\n                geom_segment(\n                    x = 0,\n                    xend = min_n_power,\n                    y = pow,\n                    yend = pow,\n                    color = segment_color,\n                    linetype = segment_linetype) +\n                ggplot2::annotate(\n                    geom = 'text',\n                    x = 0,\n                    y =  pow,\n                    label = paste0('Power = ',  pow),\n                    vjust = -1,\n                    hjust = 0) +\n                geom_segment(\n                    x = min_n_power,\n                    xend = min_n_power,\n                    y = pow,\n                    yend = 0,\n                    color = segment_color,\n                    linetype = segment_linetype) +\n                ggplot2::annotate(\n                    geom = 'text',\n                    x = min_n_power, y = 0,\n                    label = paste0('n = ', prettyNum(min_n_power, big.mark = ',')),\n                    vjust = 1,\n                    hjust = -0.1)\n        }\n        p &lt;- p +\n            geom_path(\n                aes(y = power),\n                color = '#7570b3',\n                linetype = power_linetype)\n    }\n    if(plot_p) {\n        if(any(x$sig, na.rm = TRUE)) {\n            p &lt;- p +\n                geom_segment(\n                    x = 0,\n                    xend = min(x[x$sig,]$n, na.rm = TRUE),\n                    y = attr(x, 'sig_level'),\n                    yend = attr(x, 'sig_level'),\n                    color = segment_color,\n                    linetype = segment_linetype) +\n                ggplot2::annotate(\n                    geom = 'text',\n                    x = 0,\n                    y =  attr(x, 'sig_level'),\n                    label = paste0('p = ',  attr(x, 'sig_level')),\n                    vjust = -1,\n                    hjust = 0) +\n                geom_segment(\n                    x = min(x[x$sig,]$n, na.rm = TRUE),\n                    xend = min(x[x$sig,]$n, na.rm = TRUE),\n                    y = attr(x, 'sig_level'),\n                    yend = 0,\n                    color = segment_color,\n                    linetype = segment_linetype) +\n                ggplot2::annotate(\n                    geom = 'text',\n                    x = min(x[x$sig,]$n, na.rm = TRUE),\n                    y = 0,\n                    label = paste0('n = ', prettyNum(min(x[x$sig,]$n, na.rm = TRUE), big.mark = ',')),\n                    vjust = 1,\n                    hjust = -0.1)\n        }\n        p &lt;- p +\n            geom_path(\n                alpha = 0.7,\n                linetype = p_linetype)\n            # geom_point(aes(color = sig), size = 1) +\n            # scale_color_brewer(paste0('p &lt; ', attr(x, 'sig_level')), type = 'qual', palette = 6)\n    }\n\n    if(missing(title)) {\n        if(any(x$power &gt; pow, na.rm = TRUE) & any(x$sig, na.rm = TRUE)) {\n            min_n &lt;- min(x[x$sig & x$power &gt; pow,]$n, na.rm = TRUE)\n            title &lt;- paste0('Smallest n where p &lt; ', attr(x, 'sig_level'), ' and power &gt; ', pow, ': ',\n                            prettyNum(min_n, big.mark = ','))\n        } else {\n            title &lt;- paste0('No n found where p &lt; ', attr(x, 'sig_level'), ' and power &gt; ', pow)\n        }\n    }\n\n    p &lt;- p +\n        ylim(c(0, 1)) +\n        ylab('') +\n        xlab('Sample Size') +\n        ggtitle(title,\n                subtitle = paste0('Probabilities: ', paste0(round(attr(x, 'probs'), digits = digits), collapse = ', ')))\n\n    return(p)\n}\n\nReturning to our example above where the cell proportions are 33%, 25%, and 42%, we would need \\(n \\ge 130\\) to reject the null hypothesis.\n\ncsp1 &lt;- chi_squared_power(probs =  c(.33, .25, .42))\ncsp1[csp1$sig,]$n |&gt; min(na.rm = TRUE) # Smallest n that results in p &lt; 0.05\n\n[1] 130\n\nplot(csp1)\n\n\n\n\n\n\n\n\nIn the next example we have much smaller differences between the cells with 25%, 25%, 24%, and 26%. In this example \\(n \\ge 9,710\\) before rejecting the null hypothesis.\n\ncsp3 &lt;- chi_squared_power(probs = c(.25, .25, .24, .26), max_n = 20000)\ncsp3[csp3$sig,]$n |&gt; min(na.rm = TRUE) # Smallest n that results in p &lt; 0.05\n\n[1] 9710\n\nplot(csp3)\n\n\n\n\n\n\n\n\nThis function will work with two-dimensional data as well (i.e. across two variables). The following example from Agresti (2007) looks at the political affiliation across sex (see the help documentation for chisq.test().).\n\nM &lt;- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))\ndimnames(M) &lt;- list(gender = c(\"Femal\", \"Male\"),\n                    party = c(\"Democrat\", \"Independent\", \"Republican\"))\nM\n\n       party\ngender  Democrat Independent Republican\n  Femal      762         327        468\n  Male       484         239        477\n\nsum(M)\n\n[1] 2757\n\n\nThe chi-squared test suggests we should reject the null hypothesis test.\n\nchisq.test(M)\n\n\n    Pearson's Chi-squared test\n\ndata:  M\nX-squared = 30.07, df = 2, p-value = 0.0000002954\n\nDescTools::CramerV(M) # Effect size\n\n[1] 0.1044358\n\nDescTools::power.chisq.test(n = sum(M),\n                            w = DescTools::CramerV(M),\n                            df = min(dim(M)) - 1,\n                            sig.level = 1 - sig_level)\n\n\n     Chi squared power calculation \n\n              w = 0.1044358\n              n = 2757\n             df = 1\n      sig.level = 0.05\n          power = 0.9997872\n\nNOTE: n is the number of observations\n\n\nAgresti had a sample size of 2757, but we can ask the question what is the minimum sample size would they need to detect statistical significance? First, we convert the counts to proportions, then we can use the chi_squared_power() function to find the minimum sample size to reject the null hypothesis test.\n\nM_prob &lt;- M / sum(M) # Convert the counts to percentages\ncsp4 &lt;- chi_squared_power(probs = M_prob)\nplot(csp4)\n\n\n\n\n\n\n\n\nFor a more robust application for estimating power for many statistical tests, check out the pwsrr R package and corresponding Shiny application."
  }
]